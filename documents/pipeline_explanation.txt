Explanation for the Pipeline implemented

The hardware for accelerating neural networks consists of 3 major components:
	1) DRAM based on GDDR6 technology
	2) On-chip SRAM for faster memory access
	3) NVDLA Compute engine provided by NVIDIA as an open source library

The goal of this project was to create and implement hardware pipeline that connects all 3 modules described above into one coherent system that can perform fast inference on input image given a neural network. 

For example, given an input image consisting of objects belonging to certain classes and a neural network like YOLOv3 that has been previously trained on classifiying such objects, the goal of the hardware pipeline architecture is to perform fast computation such that the objects in the image can be classified in real time. 

It is well known by now that computations involving neural networks are highly data intensive[^] and are bottlenecked at the memory and compute interface because of this nature[^]. That is to say, a lot of data needs to be moved between Memory and Compute Module, multiple times, which causes delay and slows the overall process. To counter this, we introduce a faster dynamic memory and additional SRAM stages that act as fast caches for providing compute engine the requisite data quickly with minimal latency. Using this approach and the pipeline (described below) we show that real time inference on images can be achieved for YOLOv3 and Faster R-CNN.

We use the state-of-the-art GDDR6 technology as our DRAM to aid the fast computation. In addition to this, we also use an on-chip SRAM which acts as a cache for weights and feature maps as they are computed. 


THE PIPELINE:

Before we describe the pipeline, it is important to know the stages(modules) that make up the hardware pipeline. It is shown in the diagram below	

The pipeline of the overall process is as follows :					    -------->--------	
																		|				|
																		|				|
			DRAM <--> SRAM ---- > CBUF ---- > CMAC ---- > CACC-Assembly Group 	   CACC Delivery Group ---- > SDP --- PDP --- RUBIK
						|																							|	
						|																							|			
						------------------------------------------<--------------------------------------------------																							 

DRAM 				= GDDR6 memory technology 
SRAM 				= On-chip fast memory for caching data
CBUF 				= Buffer for weights and feature data that feeds into the CMAC.
CMAC			    = Compute engine consisting of number of MAC cells for Multiplication and Addition of feature data with weights to produce partial sums 
CACC-Assembly Group = Intermediate SRAM for storing partial sums generated by CMAC
CACC-Delivery Group = Intermediate SRAM that receives data from Assembly group after 1 convolution operation and transfers data for post processing 
SDP					= Performs single data post-processing operations such as RELU, Softmax etc 
PDP                 = Performs surface operations such as Max Pooling, Average Pooling etc
RUBIK				= Performs data cube reshaping such as merge, split or contraction of data cubes

FOR YOLOV3 we use the following stages:

1)	DRAM 
2)	SRAM 
3)	CBUF
4)	CMAC
5)	CACC-Assembly Group 
6)	CACC-Delivery Group 
7)	SDP 

FOR FASTER-R-CNN we use the following stages:

1)	DRAM
2)	SRAM 
3)	CBUF
4)	CMAC
5)	CACC-Assembly Group 
6)	CACC-Delivery Group
7)	SDP 
8)	PDP 
9)	RUBIK

TODO: Describe why we use these stages for certain networks based on the operation we need to perform 

Now that we know the stages involved in the pipeline we can describe how it works

The pipeline can be split into smaller pipelines that join together to from a larger pipeline during processing as data flows between the stages. 
First we name these pipelines and draw a structure to shows how it works

	Pipe 0 -- DRAM -- > SRAM
 	Pipe 1 -- SRAM -- > CBUF 
 	Pipe 2 -- CBUF -- > CMAC -- > Assembly Group
 	Pipe 3 -- Delivery Group -- > SDP -- > SRAM
 	Pipe 4 -- Assembly Group -- > Delivery Group
 	Pipe 5 -- SRAM -- > DRAM


	 ---> L1_1_Pipe  ---->  L1_2_pipe ----> Pipe 5 ----> L1_1_Pipe ----> L1_2_pipe ----> Pipe 5 		----------->                  ---> L1_1_Pipe ----> L1_2_Pipe ----> Pipe 5
	/		 /\		            /\						   /\		         /\ 													 /
   / 		/  \		       /  \						  /  \		        /  \             Repeat until SRAM data is spent   		/			   	Repeat previous steps for all data chunks in DRAM
  /		   /	\		      /    \					 /	  \		       /    \           			...........				   /									............
 / 		  /		 \	         /      \					/	   \	      /      \												  /	
Pipe0	Pipe 1 Pipe 4      Pipe 2   Pipe 3			  Pipe 1 Pipe 4      Pipe 2   Pipe 3										Pipe0


Pipe 2 and Pipe 3 together form a top level Pipeline that we call L1_2_Pipe. While the data in the CBUF is being computed, at the same time, the data that that has been computed and now stored in Delivery Group can be moved to SDP and finally to on chip SRAM. This would mean that while the next chunk of data is being computed. The current one can be post-processed and put back in on chip SRAM for the next layer. Here we consider the Pipe which takes the maximum time to finish as the top level L1_2_Pipe time. This is beacuse the two pipes are working in parallel and we can begin the next top level operation only once this has completed. For example if Pipe 2 is faster than Pipe 3, then we consider Pipe 3s time as L1_2_Pipe time. 

Pipe 1 and Pipe 4 together form a top level Pipeline that we call L1_1_Pipe. While data is being transferred from SRAM to CBUF, at the same time, the output of the first chunk that has been computed can be transferred from Assembly Group to Delivery Group. Here we take consider the Pipe which takes the maximum time to finish. For example if Pipe 1 takes longer, we consider the time taken to finish Pipe 1 as the time taken to finish the LI_1_Pipe process. This is because, the two Pipes are working in parallel and we can only begin the next top level Pipe when this has completed. This would mean the faster of the two Pipes would have to wait until the top level Pipe has completed. 

Pipe 0 is the process of transfer of data from DRAM to SRAM which happens everytime we need to transfer data to SRAM such as weights for next layer or feature data for Concatenation in Resnet operation. 

Pipe 5 is the process of transfer of data from SRAM back to DRAM. We need to wait for this Pipe to complete before we move to top level Pipes for the next operation. We store the feature data back to DRAM for Concatentation operation or for cases when all weights cannot be transferrred to SRAM at once. So we batch weights once a batch is complete we transfer the feature data again for the new batch along with weights until all the batches have been computed. This is an important step and the time to do so needs to be added to overall inference time. We need to wait for this to complete.


It is also important to understand why the various pipes were described the way they are. During the computation process, there are two main processes involved which determine the time for inference; data movement from memory unit and computation on the data being moved. These two processes happen one after the another if we follow one data chunk being moved from main memory to the computing modules and back. Since any data chunk is essentially transferred from one memory to another memory unit (with computation process happening in between), the entire process (main pipeline) can be divided into memory-to-memory data transfers between various memory modules in this architecture. As a result, we begin to see smaller pipelines appear. However this picture is true only for any one data chunk being transferred. Hence, for just one data chunk moved from GDDR6 to Compute module and back, we will add the time taken for each pipeline described above. Since each data chunks holds finite amount of data (bits), transferring these bits in various pipes as described above  makes the process more efficient and reduces the total transfer time.
 Now, if we introduce another chunk of data we can begin to see a higher level pipeline appear which we denote as L1 level pipeline in the diagram above (Fig 18). Therefore in this picture, instead of adding the time for both chunks (as in the above case) we only need to add the time for the higher level pipeline (while it is active). This way we have smaller time of inference. This L1 pipeline becomes apparent when we see two data chunks being transferred memory-to-memory and thus it makes proper sense to view the inference process split both horizontally (multiple pipes for each data chunk) and vertically ( levels of pipeline for multiple data chunks).
So we can see how pipelining both horizontally by chunking data and vertically by introducing multiple chunks of data, we can achieve better inference time. 




Abstract:

In this report we discuss the NVDLA(open source accelerator by NVIDIA for ML inference acceleration) in conjunction with state-of-the-art GDDR6 memory technology for Neural Network acceleration. In particular, we discuss acceleration of two neural networks; YOLOv3 and Faster R-CNN which are widely used for object detection/classification and serve as good candidates for testing. This is because of the variety of layers in the networks including Convolution Layers, Fully Connected Layers, RESNET operation (Concatenation of feature maps), Max Pooling, Reshape etc which are pretty standard across several such architectures and provide sufficent complexity for hardware mapping. Mapping these networks to the hardware architecture allows us to implement a fast and efficient pipeline that could provide real time inference on these networks and networks of similar complexity. At the same time, it also allows us to examine the role of the new GDDR6 technology and on-chip SRAM in boosting our architecture. 
As a result, we created a software technology that computes the inference time for a given input image based on our pipeline architecture impelmented for a system with high level components as GDDR6 as main memory, on-chip SRAM and NVDLA Computing architecture. Keeping the technology configurable was an important decision as it gives the freedom to choose components given an input neural network and build an optimal hardware architecture. On top of this, the pipeline is built which provides fast computing and efficient use of hardware. The larger benifit of this technology is fast prototyping and design of hardware architecture for various classes of Neural Networks that can be inferred in real time for time critical tasks. 
This report is organized as follows: section 1, we will briefly introduce the NVDLA; section 2, details of memory; section 3, three modes in CBUF; section 4, pipeline; section 5, how the neural network is computed; section 6, results and conclusion.








